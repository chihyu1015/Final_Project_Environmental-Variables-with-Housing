{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50a2230-2d94-4c68-bf2b-3cf9f2115177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (1373352, 32) Test set size: (343339, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('feature_enhanced.csv')  # Adjust path if needed\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('taxvaluedollarcnt', axis=1)\n",
    "y = df['taxvaluedollarcnt']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", X_train.shape, \"Test set size:\", X_test.shape)\n",
    "\n",
    "# Baseline MAE (mean predictor)\n",
    "train_mean_value = y_train.mean()\n",
    "baseline_pred_train = np.full(len(y_train), train_mean_value)\n",
    "baseline_pred_test = np.full(len(y_test), train_mean_value)\n",
    "baseline_mae_train = mean_absolute_error(y_train, baseline_pred_train)\n",
    "baseline_mae_test = mean_absolute_error(y_test, baseline_pred_test)\n",
    "\n",
    "# Evaluation metric function\n",
    "def compute_metrics(y_true, y_pred, baseline_mae):\n",
    "    mae_val = mean_absolute_error(y_true, y_pred)\n",
    "    mse_val = mean_squared_error(y_true, y_pred)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mask = y_true != 0\n",
    "    mape_val = np.mean(np.abs((y_pred[mask] - y_true[mask]) / y_true[mask])) * 100\n",
    "    y_pred_clip = np.where(y_pred < 0, 0, y_pred)\n",
    "    y_true_clip = np.where(y_true < 0, 0, y_true)\n",
    "    msle_val = mean_squared_error(np.log1p(y_true_clip), np.log1p(y_pred_clip))\n",
    "    rmsle_val = np.sqrt(msle_val)\n",
    "    r2_val = r2_score(y_true, y_pred)\n",
    "    mase_val = mae_val / baseline_mae if baseline_mae != 0 else np.nan\n",
    "    return r2_val, mae_val, rmse_val, mape_val, mase_val, rmsle_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822dce65-3277-4667-bb23-fc0f82115f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6054\n",
      "MAE: 154806.6761\n",
      "RMSE: 268143.2136\n",
      "MAPE: 74.2822\n",
      "MASE: 0.6393\n",
      "RMSLE: 0.6613\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6080\n",
      "MAE: 154767.7234\n",
      "RMSE: 267684.5943\n",
      "MAPE: 74.0665\n",
      "MASE: 0.6372\n",
      "RMSLE: 0.6603\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93da50c1-9c2c-4c3f-8f89-15860ff4b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6105\n",
      "MAE: 153975.6794\n",
      "RMSE: 266421.6272\n",
      "MAPE: 73.9850\n",
      "MASE: 0.6359\n",
      "RMSLE: 0.6571\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6140\n",
      "MAE: 153882.5540\n",
      "RMSE: 265615.0543\n",
      "MAPE: 73.7619\n",
      "MASE: 0.6336\n",
      "RMSLE: 0.6545\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75abad82-7f64-4cfb-9220-dfd9d72fd454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6122\n",
      "MAE: 153533.5965\n",
      "RMSE: 265839.7112\n",
      "MAPE: 73.7528\n",
      "MASE: 0.6341\n",
      "RMSLE: 0.6512\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6169\n",
      "MAE: 153417.8342\n",
      "RMSE: 264637.7618\n",
      "MAPE: 73.4945\n",
      "MASE: 0.6317\n",
      "RMSLE: 0.6517\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c5d096-151e-46e0-9aec-f5ed5f7bde14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6129\n",
      "MAE: 153188.9947\n",
      "RMSE: 265599.0741\n",
      "MAPE: 73.6000\n",
      "MASE: 0.6326\n",
      "RMSLE: 0.6505\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6163\n",
      "MAE: 153130.3989\n",
      "RMSE: 264817.7169\n",
      "MAPE: 73.3474\n",
      "MASE: 0.6305\n",
      "RMSLE: 0.6492\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67089585-5a95-4da4-ac39-8878e73dad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6123\n",
      "MAE: 152928.9154\n",
      "RMSE: 265796.0971\n",
      "MAPE: 73.4038\n",
      "MASE: 0.6316\n",
      "RMSLE: 0.6501\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6174\n",
      "MAE: 152769.4566\n",
      "RMSE: 264448.5363\n",
      "MAPE: 73.1736\n",
      "MASE: 0.6290\n",
      "RMSLE: 0.6489\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad493927-0cc2-4485-aef5-6cd5d41bc03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6122\n",
      "MAE: 152713.8811\n",
      "RMSE: 265852.9580\n",
      "MAPE: 73.2504\n",
      "MASE: 0.6307\n",
      "RMSLE: 0.6482\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6153\n",
      "MAE: 152704.4090\n",
      "RMSE: 265184.4481\n",
      "MAPE: 73.0217\n",
      "MASE: 0.6287\n",
      "RMSLE: 0.6467\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3503315f-fa48-44fb-968d-b45a092023d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6112\n",
      "MAE: 152630.4855\n",
      "RMSE: 266183.8685\n",
      "MAPE: 73.1443\n",
      "MASE: 0.6303\n",
      "RMSLE: 0.6482\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6147\n",
      "MAE: 152530.8241\n",
      "RMSE: 265381.4544\n",
      "MAPE: 72.8834\n",
      "MASE: 0.6280\n",
      "RMSLE: 0.6478\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d304c-5d53-4d6b-a591-0d72777c4317",
   "metadata": {},
   "source": [
    "<h1>Find out max_depth=7 is the best among 3,4,5,6,7,8,9</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0519011a-1dc8-4693-a7a4-7c2855400346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6115\n",
      "MAE: 152925.7057\n",
      "RMSE: 266083.4401\n",
      "MAPE: 73.3899\n",
      "MASE: 0.6315\n",
      "RMSLE: 0.6485\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6164\n",
      "MAE: 152841.9559\n",
      "RMSE: 264799.4724\n",
      "MAPE: 73.1514\n",
      "MASE: 0.6293\n",
      "RMSLE: 0.6479\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=50,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69188d10-397b-4b78-b1d3-b899ec387d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6123\n",
      "MAE: 152928.9154\n",
      "RMSE: 265796.0971\n",
      "MAPE: 73.4038\n",
      "MASE: 0.6316\n",
      "RMSLE: 0.6501\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6174\n",
      "MAE: 152769.4566\n",
      "RMSE: 264448.5363\n",
      "MAPE: 73.1736\n",
      "MASE: 0.6290\n",
      "RMSLE: 0.6489\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=100,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495ed7b6-0491-4329-97ca-b8690cfdda2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6137\n",
      "MAE: 152867.9376\n",
      "RMSE: 265334.3269\n",
      "MAPE: 73.4157\n",
      "MASE: 0.6313\n",
      "RMSLE: 0.6492\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6172\n",
      "MAE: 152801.4704\n",
      "RMSE: 264530.3868\n",
      "MAPE: 73.1794\n",
      "MASE: 0.6291\n",
      "RMSLE: 0.6473\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=150,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30709abc-13a8-4a10-88c6-9e356d41dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6137\n",
      "MAE: 152916.7669\n",
      "RMSE: 265315.5550\n",
      "MAPE: 73.4281\n",
      "MASE: 0.6315\n",
      "RMSLE: 0.6495\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6175\n",
      "MAE: 152802.4631\n",
      "RMSE: 264422.5190\n",
      "MAPE: 73.1630\n",
      "MASE: 0.6291\n",
      "RMSLE: 0.6480\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=200,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2cc5744-1bac-4f46-b071-13738665691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6137\n",
      "MAE: 152913.7369\n",
      "RMSE: 265320.1593\n",
      "MAPE: 73.4200\n",
      "MASE: 0.6315\n",
      "RMSLE: 0.6493\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6178\n",
      "MAE: 152825.6109\n",
      "RMSE: 264324.0865\n",
      "MAPE: 73.1885\n",
      "MASE: 0.6292\n",
      "RMSLE: 0.6487\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e62f7a45-e9a0-4504-8bbc-b5d97fe78ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6141\n",
      "MAE: 152930.2390\n",
      "RMSE: 265187.1685\n",
      "MAPE: 73.4386\n",
      "MASE: 0.6316\n",
      "RMSLE: 0.6494\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6174\n",
      "MAE: 152827.4543\n",
      "RMSE: 264441.9659\n",
      "MAPE: 73.1782\n",
      "MASE: 0.6292\n",
      "RMSLE: 0.6479\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=300,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcf58a-49de-444d-931f-7dd344ebaa73",
   "metadata": {},
   "source": [
    "<h1>Find out min_child_weight=250 is the best among 50,100,150,200,250,300</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2cfe42c-ff3e-4bbc-8b2c-bce942cd13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6117\n",
      "MAE: 153392.1084\n",
      "RMSE: 266005.2621\n",
      "MAPE: 73.7974\n",
      "MASE: 0.6335\n",
      "RMSLE: 0.6486\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6156\n",
      "MAE: 153283.6223\n",
      "RMSE: 265074.9028\n",
      "MAPE: 73.5366\n",
      "MASE: 0.6311\n",
      "RMSLE: 0.6479\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f21b45e-7f6e-429b-9b8e-46339032402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6136\n",
      "MAE: 153065.0324\n",
      "RMSE: 265352.1162\n",
      "MAPE: 73.5705\n",
      "MASE: 0.6321\n",
      "RMSLE: 0.6483\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6176\n",
      "MAE: 153009.2346\n",
      "RMSE: 264385.1940\n",
      "MAPE: 73.3276\n",
      "MASE: 0.6300\n",
      "RMSLE: 0.6490\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64771af9-20fb-4787-96a7-9f33c91c5015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6140\n",
      "MAE: 152949.0459\n",
      "RMSE: 265230.7415\n",
      "MAPE: 73.4781\n",
      "MASE: 0.6316\n",
      "RMSLE: 0.6487\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6178\n",
      "MAE: 152857.3220\n",
      "RMSE: 264303.5414\n",
      "MAPE: 73.2251\n",
      "MASE: 0.6294\n",
      "RMSLE: 0.6483\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13505b95-2f76-444a-9526-884f2b4b573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6137\n",
      "MAE: 152913.7369\n",
      "RMSE: 265320.1593\n",
      "MAPE: 73.4200\n",
      "MASE: 0.6315\n",
      "RMSLE: 0.6493\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6178\n",
      "MAE: 152825.6109\n",
      "RMSE: 264324.0865\n",
      "MAPE: 73.1885\n",
      "MASE: 0.6292\n",
      "RMSLE: 0.6487\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a59637a7-9d3b-4218-8ffc-d52ab3bcce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6133\n",
      "MAE: 152900.7292\n",
      "RMSE: 265466.1919\n",
      "MAPE: 73.3891\n",
      "MASE: 0.6314\n",
      "RMSLE: 0.6493\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6178\n",
      "MAE: 152776.7593\n",
      "RMSE: 264328.8481\n",
      "MAPE: 73.1422\n",
      "MASE: 0.6290\n",
      "RMSLE: 0.6494\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f389654-fac0-4afe-adc2-b68f5ac46080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost CV Results =====\n",
      "R2: 0.6129\n",
      "MAE: 152896.6918\n",
      "RMSE: 265610.1693\n",
      "MAPE: 73.3583\n",
      "MASE: 0.6314\n",
      "RMSLE: 0.6490\n",
      "\n",
      "===== XGBoost Test Results =====\n",
      "R2: 0.6172\n",
      "MAE: 152774.4342\n",
      "RMSE: 264510.2125\n",
      "MAPE: 73.1289\n",
      "MASE: 0.6290\n",
      "RMSLE: 0.6501\n"
     ]
    }
   ],
   "source": [
    "#XGBoost parameters\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.10,\n",
    "    min_child_weight=250,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method='hist',\n",
    "    n_jobs=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2, cv_mae, cv_rmse, cv_mape, cv_mase, cv_rmsle = [], [], [], [], [], []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    xgb_reg.fit(X_tr, y_tr)\n",
    "    y_val_pred = xgb_reg.predict(X_val)\n",
    "    r2, mae, rmse, mape, mase, rmsle = compute_metrics(y_val, y_val_pred, baseline_mae_train)\n",
    "    cv_r2.append(r2)\n",
    "    cv_mae.append(mae)\n",
    "    cv_rmse.append(rmse)\n",
    "    cv_mape.append(mape)\n",
    "    cv_mase.append(mase)\n",
    "    cv_rmsle.append(rmsle)\n",
    "\n",
    "cv_results = {\n",
    "    \"R2\":    np.mean(cv_r2),\n",
    "    \"MAE\":   np.mean(cv_mae),\n",
    "    \"RMSE\":  np.mean(cv_rmse),\n",
    "    \"MAPE\":  np.mean(cv_mape),\n",
    "    \"MASE\":  np.mean(cv_mase),\n",
    "    \"RMSLE\": np.mean(cv_rmsle)\n",
    "}\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "r2_te, mae_te, rmse_te, mape_te, mase_te, rmsle_te = compute_metrics(\n",
    "    y_test, y_test_pred_xgb, baseline_mae_test\n",
    ")\n",
    "\n",
    "test_results = {\n",
    "    \"R2\":    r2_te,\n",
    "    \"MAE\":   mae_te,\n",
    "    \"RMSE\":  rmse_te,\n",
    "    \"MAPE\":  mape_te,\n",
    "    \"MASE\":  mase_te,\n",
    "    \"RMSLE\": rmsle_te\n",
    "}\n",
    "\n",
    "print(\"\\n===== XGBoost CV Results =====\")\n",
    "for k, v in cv_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== XGBoost Test Results =====\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666f65b-befc-4220-a611-04a514eb8e99",
   "metadata": {},
   "source": [
    "<h1>Find out n_estimators = 300 is the best among 100,200,300,400,500,600</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f3a9a-f04c-4b5b-8aa2-c8f01d8a72f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
